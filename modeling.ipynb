{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-train-data\" data-toc-modified-id=\"Loading-train-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading train data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merging-train-and-test-data\" data-toc-modified-id=\"Merging-train-and-test-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Merging train and test data</a></span></li><li><span><a href=\"#Merging-user-data\" data-toc-modified-id=\"Merging-user-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merging user data</a></span></li><li><span><a href=\"#Competitions-based-features\" data-toc-modified-id=\"Competitions-based-features-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Competitions based features</a></span></li><li><span><a href=\"#Competitions-data\" data-toc-modified-id=\"Competitions-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Competitions data</a></span></li><li><span><a href=\"#Time-based-competitions-features\" data-toc-modified-id=\"Time-based-competitions-features-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Time based competitions features</a></span></li><li><span><a href=\"#Current-active-competitions-feature\" data-toc-modified-id=\"Current-active-competitions-feature-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Current active competitions feature</a></span></li><li><span><a href=\"#User-Interests-Feature\" data-toc-modified-id=\"User-Interests-Feature-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>User Interests Feature</a></span></li><li><span><a href=\"#Submissions-based-features\" data-toc-modified-id=\"Submissions-based-features-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Submissions based features</a></span></li><li><span><a href=\"#Discussion-based-features\" data-toc-modified-id=\"Discussion-based-features-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Discussion based features</a></span></li><li><span><a href=\"#Comments-based-features\" data-toc-modified-id=\"Comments-based-features-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Comments based features</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Submission</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from category_encoders import CountEncoder\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "plt.rcParams[\"ytick.labelsize\"] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Config:\n",
    "#     LAG = 3\n",
    "#     VER = f'final_sub_v1'\n",
    "#     OUTPUT_DIR = '.'\n",
    "#     DATA_DIR = '.'\n",
    "#     DEBUG = True\n",
    "#     N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL_TEST_RUN = False\n",
    "seed = 0\n",
    "# def seed_everything(seed=0):\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(df):\n",
    "    target = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['CompPart'] == 1:\n",
    "            target.append('CompPart')\n",
    "            continue\n",
    "        elif row['Sub'] == 1 or row['Comment'] == 1 or row['Disc'] == 1:\n",
    "            target.append('Sub')\n",
    "            continue\n",
    "        else:\n",
    "            target.append('NoActivity')\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./Train.csv\", index_col=None)\n",
    "print(train.shape)\n",
    "train['Target'] = target(train)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./Test.csv\", index_col=None)\n",
    "print(test.shape)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_timestamp(df):\n",
    "    year_month = df['year'].astype(str) + \\\n",
    "        df['month'].apply(lambda x: str(x).zfill(2))\n",
    "    df['year_month'] = year_month.astype(int)\n",
    "    df = df.sort_values(by='year_month').reset_index(drop=True)\n",
    "    df['timestamp'] = np.arange(1, len(df) + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "\n",
    "data = train.append(test, ignore_index=True)\n",
    "timestamp = data[['year', 'month']].drop_duplicates()\n",
    "timestamp = define_timestamp(timestamp)\n",
    "data = data.merge(timestamp, how='left')\n",
    "all_timestamps = data[['User_ID', 'timestamp', 'year', 'month']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['Record'] = 1\n",
    "data['Total_Num_User_Months'] = data.groupby('User_ID')['Record'].apply(lambda x: x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"./Users.csv\", index_col=None)\n",
    "users.columns = ['User_ID', 'FeatureX', 'Country', 'FeatureY', 'Points', 'year', 'month', 'dayofweek']\n",
    "users = users.merge(timestamp, how='left')\n",
    "users = users.rename(columns={\"timestamp\": \"Zindi_Joining_Timestamp\"})\n",
    "users.drop(['dayofweek', 'year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(users, how='left')\n",
    "\n",
    "columns = ['FeatureX', 'Country', 'FeatureY', 'Points']\n",
    "data[columns] = data[columns].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitions based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_data = pd.read_csv(\"./CompetitionPartipation.csv\", index_col=None)\n",
    "competition_data.columns = [\n",
    "    'CompID', 'User_ID', 'PublicRank', 'Successful_Sub_Count',\n",
    "    'year', 'month', 'dayofweek'\n",
    "]\n",
    "    \n",
    "competition_timestamp = competition_data.merge(timestamp, how='left')\n",
    "competition_timestamp = competition_timestamp[[\n",
    "    'User_ID', 'month', 'year', 'timestamp'\n",
    "]].drop_duplicates()\n",
    "competition_timestamp.columns = [\n",
    "    'User_ID', 'month', 'year', 'comp_timestamp'\n",
    "]\n",
    "data = data.merge(competition_timestamp, how='left')\n",
    "\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['comp_timestamp'] = data.groupby('User_ID')['comp_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "data['Months_Since_Last_Comp'] = data['timestamp'] - data['comp_timestamp']\n",
    "data['Months_Since_Joining_Zindi'] = data['comp_timestamp'] - data['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions = pd.read_csv(\"./Competitions.csv\", index_col=None, skipinitialspace=True)\n",
    "competitions['CompEndTime Year'] = [\n",
    "    int(val) if val != 'not mapped' else 999 \\\n",
    "    for val in competitions['CompEndTime Year']\n",
    "]\n",
    "competitions['FeatureC'] = competitions['FeatureC'].fillna(-1).astype(np.int8)\n",
    "competitions = competitions.merge(\n",
    "    timestamp,\n",
    "    left_on=['CompStartTime Year', 'CompStartTime Month'],\n",
    "    right_on=['year', 'month'],\n",
    "    how='left')\n",
    "competitions = competitions.rename(columns={\n",
    "    'timestamp': 'comp_start_timestamp',\n",
    "})\n",
    "competitions.drop(['year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "competitions = competitions.merge(\n",
    "    timestamp,\n",
    "    left_on=['CompEndTime Year', 'CompEndTime Month'],\n",
    "    right_on=['year', 'month'],\n",
    "    how='left')\n",
    "competitions = competitions.rename(columns={\n",
    "    'timestamp': 'comp_end_timestamp',\n",
    "})\n",
    "competitions['comp_end_timestamp'] = competitions['comp_end_timestamp'].fillna(99)\n",
    "competitions.drop(['year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "competitions['comp_duration'] = competitions['comp_end_timestamp'] - competitions['comp_start_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['FeatureA', 'FeatureB', 'FeatureE']:\n",
    "    competitions[column] = competitions[column].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = competitions[['CompID']].copy()\n",
    "for column in ['FeatureA', 'FeatureB', 'FeatureC', 'FeatureD', 'FeatureE']:\n",
    "    table = competitions[['CompID', column]].explode(column=[column])\n",
    "    table[column] = table[column].fillna('empty')\n",
    "    table['count'] = 1\n",
    "\n",
    "    table = table.pivot_table(\n",
    "        index='CompID', \n",
    "        columns=column,\n",
    "        values='count',\n",
    "        aggfunc='count'\n",
    "    )\n",
    "    table.columns = [table.columns.name + \"_\" + str(cl) for cl in table.columns]\n",
    "    table = table.reset_index()\n",
    "    features = features.merge(table, how='left')\n",
    "features = features.fillna(0)\n",
    "features = features.merge(competitions[['CompID', 'comp_start_timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time based competitions features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_inds = []\n",
    "competitions_inds = []\n",
    "\n",
    "for t in tqdm(timestamp.timestamp):\n",
    "    current_competition = competitions.CompID[\n",
    "        (t>=competitions.comp_start_timestamp) & (t<=competitions.comp_end_timestamp)\n",
    "    ]\n",
    "    timestamp_inds.extend([t] * len(current_competition))\n",
    "    competitions_inds.extend(current_competition)\n",
    "\n",
    "timestamp_competition = pd.DataFrame({\n",
    "    \"timestamp\": timestamp_inds,\n",
    "    \"CompID\": competitions_inds\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current active competitions feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_competition = competition_data.merge(timestamp, how='left')\n",
    "current_competition = current_competition.rename(columns={\"timestamp\": \"comp_timestamp\"})\n",
    "current_competition = current_competition[\n",
    "    ['User_ID', 'CompID', 'comp_timestamp']\n",
    "].merge(competitions[\n",
    "    ['CompID', 'comp_start_timestamp', 'comp_end_timestamp']\n",
    "], how='left')\n",
    "current_competition = current_competition[current_competition['comp_end_timestamp']!=99].reset_index(drop=True)\n",
    "current_competition = all_timestamps.merge(current_competition, how='left')\n",
    "\n",
    "current_competition['Current_Active_Competitions'] = (\n",
    "    (current_competition['timestamp'] > current_competition['comp_timestamp']) &\n",
    "    (current_competition['timestamp'] <= current_competition['comp_end_timestamp'])\n",
    ").astype(np.int8)\n",
    "\n",
    "current_competition = current_competition.groupby(['User_ID', 'timestamp'])['Current_Active_Competitions'].sum()\n",
    "current_competition = current_competition.reset_index()\n",
    "\n",
    "data = data.merge(current_competition, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interests Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_competition = timestamp_competition.merge(features, how='left')\n",
    "timestamp_competition.drop(['comp_start_timestamp', 'CompID'], axis=1, inplace=True)\n",
    "timestamp_competition = timestamp_competition.groupby('timestamp').agg(np.sum).reset_index()\n",
    "timestamp_competition = timestamp_competition.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "competition_timestamp = competition_data.merge(timestamp, how='left')\n",
    "usr_features = competition_timestamp[['User_ID', 'CompID', 'timestamp']].merge(features, how='left')\n",
    "usr_features = usr_features.drop(['CompID', 'comp_start_timestamp'], axis=1)\n",
    "usr_features = usr_features.groupby(['User_ID', 'timestamp']).agg(np.sum)#.groupby(level=0).cumsum()\n",
    "usr_features = usr_features.reset_index()\n",
    "usr_features = all_timestamps.merge(usr_features, how='left')\n",
    "columns = usr_features.columns[4:]\n",
    "print(columns)\n",
    "\n",
    "usr_features = usr_features.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for cl in tqdm(columns):\n",
    "    usr_features[cl] = usr_features.groupby('User_ID')[cl].apply(lambda x: x.ffill())\n",
    "\n",
    "usr_features = usr_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "timestamp_inds = []\n",
    "usr_inds = []\n",
    "user_interests = []\n",
    "\n",
    "for t in timestamp_competition['timestamp']:\n",
    "    if t == 1:\n",
    "        continue\n",
    "    usr_filtered = usr_features[usr_features['timestamp']==t-1]\n",
    "    timestamp_inds.extend([t]*len(usr_filtered))\n",
    "    usr_inds.extend(usr_filtered.pop('User_ID'))\n",
    "    \n",
    "    comp_filtered = timestamp_competition[timestamp_competition['timestamp']==t]\n",
    "    usr_filtered.drop(['timestamp', 'year', 'month'], axis=1, inplace=True)\n",
    "    comp_filtered.drop('timestamp', axis=1, inplace=True)\n",
    "    \n",
    "    interests = np.matmul(usr_filtered.values, comp_filtered.values.T).flatten()\n",
    "#     interests = cosine_similarity(usr_filtered.values, comp_filtered.values).flatten()\n",
    "    user_interests.extend(interests)\n",
    "\n",
    "usr_interest_f = pd.DataFrame({\n",
    "    \"timestamp\": timestamp_inds,\n",
    "    \"User_ID\": usr_inds,\n",
    "    \"user_interests\": user_interests\n",
    "})\n",
    "\n",
    "data = data.merge(usr_interest_f, how='left')\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['user_interests'] = data.groupby('User_ID')['user_interests'].apply(lambda x: x.ffill())\n",
    "data['user_interests'] = data['user_interests'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = competition_data.groupby(['User_ID', 'year', 'month'])['CompID'].nunique()\n",
    "history = history.reset_index()\n",
    "history.columns = [*history.columns[:-1]] + ['Num_Comp_Prev_Month']\n",
    "\n",
    "data = data.merge(history, how='left')\n",
    "data = data.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "data['Num_Comp_Prev_Month'] = data['Num_Comp_Prev_Month'].fillna(0)\n",
    "data['Num_Comp_Per_Month'] = data.groupby('User_ID')['Num_Comp_Prev_Month'].cumsum()\n",
    "data['Num_Comp_Per_Month_trend'] = data['Num_Comp_Per_Month']/data['Total_Num_User_Months']\n",
    "data['Num_Comp_Per_Month_trend'] = data.groupby('User_ID')['Num_Comp_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "data['Num_Comp_Per_Month'] = data['Num_Comp_Per_Month']/(data['timestamp'].max() - data['Zindi_Joining_Timestamp'])\n",
    "data['Num_Comp_Per_Month'] = data.groupby('User_ID')['Num_Comp_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "data['Num_Comp_Prev_Month'] = data.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift())\n",
    "data['Num_Comp_Prev_Month_momentum'] = data['Num_Comp_Prev_Month'] - data.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "data['Num_Comp_Prev_Month_momentum2'] = data['Num_Comp_Prev_Month'] - data.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = competition_data.groupby(['User_ID', 'year', 'month', 'PublicRank'])['CompID'] \\\n",
    "    .nunique().unstack('PublicRank').apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [table.columns.name + \"_\" + str(cl) for cl in table.columns]\n",
    "table.columns = col_names\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()\n",
    "\n",
    "table = all_timestamps.merge(table, how='left')\n",
    "table = table.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    table[col] = table.groupby('User_ID')[col].apply(lambda x: x.cumsum().ffill().shift())\n",
    "    \n",
    "data = data.merge(table, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = competition_data.groupby(['User_ID', 'year', 'month', 'Successful_Sub_Count'])['CompID'].nunique().unstack('Successful_Sub_Count').apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [table.columns.name + \"_\" + str(col) for col in table.columns]\n",
    "table.columns = col_names\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()\n",
    "\n",
    "table = all_timestamps.merge(table, how='left')\n",
    "table = table.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    table[col] = table.groupby('User_ID')[col].apply(lambda x: x.cumsum().ffill().shift())\n",
    "    \n",
    "data = data.merge(table, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del competition_data, competition_timestamp, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./Submissions.csv\", index_col=None)\n",
    "submission.columns = ['User_ID', 'FeatureG', 'CompID', 'year', 'month', 'dayofweek']\n",
    "    \n",
    "submission_timestamp = submission.merge(timestamp, how='left')\n",
    "submission_timestamp = submission_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "submission_timestamp.columns = ['User_ID', 'month', 'year', 'sub_timestamp']\n",
    "data = data.merge(submission_timestamp, how='left')\n",
    "\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['sub_timestamp'] = data.groupby('User_ID')['sub_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "data['Months_Since_Last_Sub'] = data['timestamp'] - data['sub_timestamp']\n",
    "data['Months_Since_Sub_Joining_Zindi'] = data['sub_timestamp'] - data['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_submission = submission.groupby(['User_ID', 'year', 'month']).agg({'CompID': ['nunique', 'count']})\n",
    "unique_submission.columns = [\"_\".join(col) for col in unique_submission.columns]\n",
    "unique_submission['Sub_Per_Comp'] = unique_submission['CompID_nunique']/unique_submission['CompID_count']\n",
    "unique_submission.drop(['CompID_nunique', 'CompID_count'], axis=1, inplace=True)\n",
    "unique_submission = unique_submission.reset_index()\n",
    "unique_submission.columns = [*unique_submission.columns[:-1]] + ['Num_Sub_Prev_Month']\n",
    "\n",
    "data = data.merge(unique_submission, how='left')\n",
    "data = data.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "data['Num_Sub_Prev_Month'] = data['Num_Sub_Prev_Month'].fillna(0)\n",
    "data['Num_Sub_Per_Month'] = data.groupby('User_ID')['Num_Sub_Prev_Month'].cumsum()\n",
    "data['Num_Sub_Per_Month_trend'] = data['Num_Sub_Per_Month']/data['Total_Num_User_Months']\n",
    "data['Num_Sub_Per_Month_trend'] = data.groupby('User_ID')['Num_Sub_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "data['Num_Sub_Per_Month'] = data['Num_Sub_Per_Month']/(data['timestamp'].max() - data['Zindi_Joining_Timestamp'])\n",
    "data['Num_Sub_Per_Month'] = data.groupby('User_ID')['Num_Sub_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "data['Num_Sub_Prev_Month'] = data.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift())\n",
    "data['Num_Sub_Prev_Month_momentum'] = data['Num_Sub_Prev_Month'] - data.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "data['Num_Sub_Prev_Month_momentum2'] = data['Num_Sub_Prev_Month'] - data.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>sub_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_8JP75F20</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ID_8JP75F20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ID_8JP75F20</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>ID_8JP75F20</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>ID_8JP75F20</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User_ID  month  year  sub_timestamp\n",
       "0    ID_8JP75F20      3     3             24\n",
       "34   ID_8JP75F20      1     3             22\n",
       "51   ID_8JP75F20      4     3             25\n",
       "123  ID_8JP75F20      5     3             26\n",
       "233  ID_8JP75F20     12     2             21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_timestamp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FeatureG'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\acer\\Documents\\MyDocs\\Programming\\VisualCode\\ds_project_intro\\modeling.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/acer/Documents/MyDocs/Programming/VisualCode/ds_project_intro/modeling.ipynb#Y320sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m table \u001b[39m=\u001b[39m submission_timestamp\u001b[39m.\u001b[39mgroupby([\u001b[39m'\u001b[39m\u001b[39mUser_ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmonth\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFeatureG\u001b[39m\u001b[39m'\u001b[39m])[\u001b[39m'\u001b[39m\u001b[39mCompID\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnunique()\u001b[39m.\u001b[39munstack(\u001b[39m'\u001b[39m\u001b[39mFeatureG\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m#.apply(lambda x: x/x.sum(), axis=1)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/Documents/MyDocs/Programming/VisualCode/ds_project_intro/modeling.ipynb#Y320sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m col_names \u001b[39m=\u001b[39m [table\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(col) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m table\u001b[39m.\u001b[39mcolumns]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/Documents/MyDocs/Programming/VisualCode/ds_project_intro/modeling.ipynb#Y320sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m table\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m col_names\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\acer\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FeatureG'"
     ]
    }
   ],
   "source": [
    "table = submission_timestamp.groupby(['User_ID', 'year', 'month', 'FeatureG'])['CompID'].nunique().unstack('FeatureG')#.apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [table.columns.name + \"_\" + str(col) for col in table.columns]\n",
    "table.columns = col_names\n",
    "table = table.fillna(0)\n",
    "table = table.reset_index()\n",
    "\n",
    "all_timestamps = data[['User_ID', 'timestamp', 'year', 'month']].drop_duplicates().reset_index(drop=True)\n",
    "table = all_timestamps.merge(table, how='left')\n",
    "table = table.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    table[col] = table.groupby('User_ID')[col].apply(lambda x: x.ffill().shift())\n",
    "    \n",
    "data = data.merge(table, how='left')\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del submission, submission_timestamp, unique_submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dis = pd.read_csv(\"./Discussions.csv\", index_col=None)\n",
    "usr_dis.columns = ['FeatureF', 'year', 'month', 'dayofweek', 'DiscID', 'User_ID']\n",
    "    \n",
    "usr_dis_timestamp = usr_dis.merge(timestamp, how='left')\n",
    "usr_dis_timestamp = usr_dis_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_dis_timestamp.columns = ['User_ID', 'month', 'year', 'discussion_timestamp']\n",
    "data = data.merge(usr_dis_timestamp, how='left')\n",
    "\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['discussion_timestamp'] = data.groupby('User_ID')['discussion_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "data['Months_Since_Last_Dis'] = data['timestamp'] - data['discussion_timestamp']\n",
    "data['Months_Since_Dis_Joining_Zindi'] = data['discussion_timestamp'] - data['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_hist = usr_dis.groupby(['User_ID', 'year', 'month'])['DiscID'].nunique()\n",
    "dis_hist = dis_hist.reset_index()\n",
    "dis_hist.columns = [*dis_hist.columns[:-1]] + ['Num_Dis_Prev_Month']\n",
    "\n",
    "data = data.merge(dis_hist, how='left')\n",
    "data = data.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "data['Num_Dis_Prev_Month'] = data['Num_Dis_Prev_Month'].fillna(0)\n",
    "data['Num_Dis_Per_Month'] = data.groupby('User_ID')['Num_Dis_Prev_Month'].cumsum()\n",
    "data['Num_Dis_Per_Month_trend'] = data['Num_Dis_Per_Month']/data['Total_Num_User_Months']\n",
    "data['Num_Dis_Per_Month_trend'] = data.groupby('User_ID')['Num_Dis_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "data['Num_Dis_Per_Month'] = data['Num_Dis_Per_Month']/(data['timestamp'].max() - data['Zindi_Joining_Timestamp'])\n",
    "data['Num_Dis_Per_Month'] = data.groupby('User_ID')['Num_Dis_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "data['Num_Dis_Prev_Month'] = data.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift())\n",
    "data['Num_Dis_Prev_Month_momentum'] = data['Num_Dis_Prev_Month'] - data.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "data['Num_Dis_Prev_Month_momentum2'] = data['Num_Dis_Prev_Month'] - data.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_dis, usr_dis_timestamp, dis_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_comments = pd.read_csv(\"./Comments.csv\", index_col=None)\n",
    "usr_comments.columns = ['User_ID', 'year', 'month', 'dayofweek']\n",
    "usr_comments['CommID'] = np.arange(len(usr_comments))\n",
    "    \n",
    "usr_comm_timestamp = usr_comments.merge(timestamp, how='left')\n",
    "usr_comm_timestamp = usr_comm_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_comm_timestamp.columns = ['User_ID', 'month', 'year', 'comment_timestamp']\n",
    "data = data.merge(usr_comm_timestamp, how='left')\n",
    "\n",
    "data = data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "data['comment_timestamp'] = data.groupby('User_ID')['comment_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "data['Months_Since_Last_Comment'] = data['timestamp'] - data['comment_timestamp']\n",
    "data['Months_Since_Comment_Joining_Zindi'] = data['comment_timestamp'] - data['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_hist = usr_comments.groupby(['User_ID', 'year', 'month'])['CommID'].nunique()\n",
    "comm_hist = comm_hist.reset_index()\n",
    "comm_hist.columns = [*comm_hist.columns[:-1]] + ['Num_Comm_Prev_Month']\n",
    "\n",
    "data = data.merge(comm_hist, how='left')\n",
    "data = data.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "data['Num_Comm_Prev_Month'] = data['Num_Comm_Prev_Month'].fillna(0)\n",
    "data['Num_Comm_Per_Month'] = data.groupby('User_ID')['Num_Comm_Prev_Month'].cumsum()\n",
    "data['Num_Comm_Per_Month_trend'] = data['Num_Comm_Per_Month']/data['Total_Num_User_Months']\n",
    "data['Num_Comm_Per_Month_trend'] = data.groupby('User_ID')['Num_Comm_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "data['Num_Comm_Per_Month'] = data['Num_Comm_Per_Month']/(data['timestamp'].max() - data['Zindi_Joining_Timestamp'])\n",
    "data['Num_Comm_Per_Month'] = data.groupby('User_ID')['Num_Comm_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "data['Num_Comm_Prev_Month'] = data.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift())\n",
    "data['Num_Comm_Prev_Month_momentum'] = data['Num_Comm_Prev_Month'] - data.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "data['Num_Comm_Prev_Month_momentum2'] = data['Num_Comm_Prev_Month'] - data.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_comments, usr_comm_timestamp, comm_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.groupby('timestamp').agg({\n",
    "    \"User_ID\": [\"nunique\"],\n",
    "    \"Total_Num_User_Months\": [\"mean\", \"max\", \"std\"],\n",
    "})\n",
    "tmp.columns = [\"_\".join(col) for col in tmp.columns]\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "data = data.merge(tmp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cols = ['Months_Since_Last_Comp', 'Months_Since_Last_Dis', 'Months_Since_Last_Sub', 'Months_Since_Last_Comment']\n",
    "data['Months_Since_Last_Activity_Mean'] = data[sel_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = [\n",
    "    'Zindi_Joining_Timestamp',\n",
    "    'comment_timestamp',\n",
    "    'comp_timestamp',\n",
    "    'discussion_timestamp',\n",
    "    'sub_timestamp',\n",
    "    'Months_Since_Last_Comp',\n",
    "    'Months_Since_Last_Sub',\n",
    "    'Months_Since_Last_Dis',\n",
    "    'Months_Since_Last_Comment',\n",
    "]\n",
    "\n",
    "for col in time_cols:\n",
    "    data[col] = data[col]/data['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_time = data[data['Zindi_Joining_Timestamp']==1]\n",
    "tmp_time = tmp_time.groupby('timestamp')['User_ID'].nunique().to_frame(\"unique_user_count\")\n",
    "tmp_time = tmp_time.reset_index()\n",
    "\n",
    "data = data.merge(tmp_time, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user_interests_rank'] = data.groupby('timestamp')['user_interests'].apply(lambda x: \n",
    "                                                                                      x.rank(method='dense', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['user_interests']==0, 'user_interests'] = np.NaN\n",
    "print(train.shape, test.shape)\n",
    "train, test = data[data['is_train']==1], data[data['is_train']==0]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df_trainX, df_trainY, df_evalX, df_evalY, cat_cols, model_name='CAT', params=None):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if model_name == 'CAT':\n",
    "        if params is None:\n",
    "            params={'n_estimators':10000,'random_state':123,'cat_features':cat_cols}\n",
    "        clf=CatBoostClassifier(**params,early_stopping_rounds=50,eval_metric='AUC')\n",
    "        clf.fit(df_trainX,df_trainY,eval_set=(df_evalX,df_evalY),plot=False, verbose=50)\n",
    "        valid_score = clf.get_best_score().get('validation').get('AUC')\n",
    "        best_iteration = clf.get_best_iteration()\n",
    "        feature_score = clf.get_feature_importance()\n",
    "    elif model_name == 'LGB':\n",
    "        if params is None:\n",
    "            params={'verbose':0,'n_estimators':10000,'random_state':123,'learning_rate':0.01,'force_row_wise':True,'colsample_bytree':0.3}\n",
    "        clf = lgb.LGBMClassifier(**params, importance_type='gain', metric='auc_mu', num_leaves=127, min_child_samples=5)\n",
    "        callbacks = [lgb.early_stopping(500, verbose=0)]\n",
    "        clf.fit(df_trainX,\n",
    "                df_trainY,#)\n",
    "                eval_set=[(df_evalX, df_evalY)],\n",
    "                callbacks=callbacks,\n",
    "                # verbose=0\n",
    "               )\n",
    "\n",
    "        valid_score = roc_auc_score(df_evalY!='NoActivity', 1-clf.predict_proba(df_evalX)[:,1])\n",
    "        best_iteration = clf.booster_.best_iteration\n",
    "        feature_score = clf.feature_importances_\n",
    "    return clf, valid_score, best_iteration, feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\";'Train_fe.csv.gz\", compression='gzip')\n",
    "test.to_csv(\"./Test_fe.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drop_cols = [\n",
    "    'year', 'month', 'Target', 'Sub', 'CompPart', 'Comment', 'Disc',\n",
    "    'is_train', 'timestamp', 'Record', 'Active_Month', 'Total_Num_User_Months',\n",
    "    'user_interests'\n",
    "]\n",
    "cat_cols = list(\n",
    "    set(train.columns[train.dtypes == 'object']) - set(drop_cols) - set(['User_ID'])\n",
    ")\n",
    "num_cols = list(set(train.columns) - set(cat_cols + drop_cols))\n",
    "\n",
    "train_X = train[cat_cols + num_cols]\n",
    "train_X[cat_cols] = train_X[cat_cols].astype('category')\n",
    "train_Y = train['Target']\n",
    "\n",
    "test_X = test[cat_cols + num_cols]\n",
    "test_X[cat_cols] = test_X[cat_cols].astype('category')\n",
    "\n",
    "fold = GroupKFold(n_splits=5)\n",
    "cb_scores, pred_cb, feat_scores = [], [], []\n",
    "for it, (idxT, idxV) in enumerate(\n",
    "        fold.split(train_X, train_Y, groups=train['timestamp'])):\n",
    "    df_trainX, df_trainY = train_X.iloc[idxT], train_Y.iloc[idxT]\n",
    "    df_evalX, df_evalY = train_X.iloc[idxV], train_Y.iloc[idxV]\n",
    "    df_testX = test_X.copy()\n",
    "\n",
    "    selected_cat_cols = ['Country']\n",
    "    cat_cols_count = [f'{col}_count' for col in selected_cat_cols]\n",
    "    df_trainX[cat_cols_count] = df_trainX[selected_cat_cols].copy()\n",
    "    df_evalX[cat_cols_count] = df_evalX[selected_cat_cols].copy()\n",
    "    df_testX[cat_cols_count] = df_testX[selected_cat_cols].copy()\n",
    "\n",
    "    encoder = CountEncoder(cols=cat_cols_count + ['User_ID'])\n",
    "    df_trainX = encoder.fit_transform(df_trainX, df_trainY)\n",
    "    df_evalX = encoder.transform(df_evalX)\n",
    "    df_testX = encoder.transform(df_testX)\n",
    "\n",
    "    clf, valid_score, best_iteration, feature_score = train_model(\n",
    "        df_trainX, df_trainY, df_evalX, df_evalY, cat_cols, model_name='LGB')\n",
    "    cb_scores.append(valid_score)\n",
    "    pred_cb.append(clf.predict_proba(df_testX)[:, 1])\n",
    "    feat_scores.append(feature_score)\n",
    "    print('Fold {} {} at {}'.format(it + 1, valid_score, best_iteration))\n",
    "\n",
    "weights = cb_scores / sum(np.array(cb_scores))\n",
    "print('The local CV is {}'.format(np.sum(weights * cb_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LOCAL_TEST_RUN:\n",
    "#     weights=cb_scores/sum(np.array(cb_scores))\n",
    "#     print ('The local CV is {}'.format(np.sum(weights*cb_scores)))\n",
    "\n",
    "#     prediction = np.sum(weights*np.transpose(pred_cb),1)\n",
    "#     from sklearn.metrics import roc_auc_score\n",
    "#     print(\"Test score is {}\".format(roc_auc_score(test['Target']!='NoActivity', 1-prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featureImp=pd.DataFrame({'feature':df_trainX.columns,'importance':np.mean(np.array(feat_scores),0)})\n",
    "featureImp=featureImp.sort_values('importance',ascending=False)\n",
    "featureImp['importance']=featureImp['importance']*100/featureImp['importance'].sum()\n",
    "featureImp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Target'] = np.sum(weights * np.transpose(pred_cb), 1)\n",
    "test['Target'] = 1 - test['Target']\n",
    "test['UserMonthYear'] = test['User_ID'] + \"_\" + test['month'].astype(str) + \"_\" + test['year'].astype(str)\n",
    "test[['UserMonthYear', 'Target']].to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['UserMonthYear', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
