{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-train-data\" data-toc-modified-id=\"Loading-train-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading train data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merging-train-and-test-data\" data-toc-modified-id=\"Merging-train-and-test-data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Merging train and test data</a></span></li><li><span><a href=\"#Merging-user-data\" data-toc-modified-id=\"Merging-user-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Merging user data</a></span></li><li><span><a href=\"#Competitions-based-features\" data-toc-modified-id=\"Competitions-based-features-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Competitions based features</a></span></li><li><span><a href=\"#Competitions-data\" data-toc-modified-id=\"Competitions-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Competitions data</a></span></li><li><span><a href=\"#Time-based-competitions-features\" data-toc-modified-id=\"Time-based-competitions-features-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Time based competitions features</a></span></li><li><span><a href=\"#Current-active-competitions-feature\" data-toc-modified-id=\"Current-active-competitions-feature-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Current active competitions feature</a></span></li><li><span><a href=\"#User-Interests-Feature\" data-toc-modified-id=\"User-Interests-Feature-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>User Interests Feature</a></span></li><li><span><a href=\"#Submissions-based-features\" data-toc-modified-id=\"Submissions-based-features-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Submissions based features</a></span></li><li><span><a href=\"#Discussion-based-features\" data-toc-modified-id=\"Discussion-based-features-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Discussion based features</a></span></li><li><span><a href=\"#Comments-based-features\" data-toc-modified-id=\"Comments-based-features-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Comments based features</a></span></li></ul></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Submission</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 14\n",
    "plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from catboost import CatBoostClassifier\n",
    "from category_encoders import CountEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    LAG = 3\n",
    "    VER = f'final_sub_v1'\n",
    "    OUTPUT_DIR = '.'\n",
    "    DATA_DIR = '.'\n",
    "    DEBUG = True\n",
    "    N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_TEST_RUN = False\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_target(df):\n",
    "    new_target = []\n",
    "    for i, row in df.iterrows():\n",
    "        if row['CompPart'] == 1:\n",
    "            new_target.append('CompPart')\n",
    "            continue\n",
    "        elif row['Sub'] == 1 or row['Comment'] == 1 or row['Disc'] == 1:\n",
    "            new_target.append('Sub')\n",
    "            continue\n",
    "        else:\n",
    "            new_target.append('NoActivity')\n",
    "        \n",
    "    return new_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(Config.DATA_DIR,\"Train.csv\"), index_col=None)\n",
    "print(train.shape)\n",
    "train['Target'] = determine_target(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LOCAL_TEST_RUN:\n",
    "    test_index = (train['year']==3)&(train['month'].isin([10,11,12]))\n",
    "    test = train[test_index].reset_index(drop=True)\n",
    "    train = train[~test_index].reset_index(drop=True)\n",
    "else:\n",
    "    test = pd.read_csv(os.path.join(Config.DATA_DIR,\"Test.csv\"), index_col=None)\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_timestamp(df):\n",
    "    df['year_month'] = (\n",
    "        df['year'].astype(str) +\n",
    "        df['month'].apply(lambda x: str(x).zfill(2))\n",
    "    ).astype(int)\n",
    "    df = df.sort_values(by='year_month').reset_index(drop=True)\n",
    "    df['timestamp'] = np.arange(1, len(df) + 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "\n",
    "overall = train.append(test, ignore_index=True)\n",
    "timestamp = overall[['year', 'month']].drop_duplicates()\n",
    "timestamp = determine_timestamp(timestamp)\n",
    "overall = overall.merge(timestamp, how='left')\n",
    "all_timestamps = overall[['User_ID', 'timestamp', 'year', 'month']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['Record'] = 1\n",
    "overall['Total_Num_User_Months'] = overall.groupby('User_ID')['Record'].apply(lambda x: x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(os.path.join(Config.DATA_DIR,\"Users.csv\"), index_col=None)\n",
    "users.columns = ['User_ID', 'FeatureX', 'Country', 'FeatureY', 'Points', 'year', 'month', 'dayofweek']\n",
    "users = users.merge(timestamp, how='left')\n",
    "users = users.rename(columns={\"timestamp\": \"Zindi_Joining_Timestamp\"})\n",
    "users.drop(['dayofweek', 'year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = overall.merge(users, how='left')\n",
    "\n",
    "sel_cols = ['FeatureX', 'Country', 'FeatureY', 'Points']\n",
    "overall[sel_cols] = overall[sel_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitions based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_comp = pd.read_csv(os.path.join(Config.DATA_DIR,\"CompetitionPartipation.csv\"), index_col=None)\n",
    "usr_comp.columns = ['CompID', 'User_ID', 'PublicRank', 'Successful_Sub_Count',\n",
    "                    'year', 'month', 'dayofweek']\n",
    "if LOCAL_TEST_RUN:\n",
    "    test_index = (usr_comp['year']==3)&(usr_comp['month'].isin([10,11,12]))\n",
    "    usr_comp = usr_comp[~test_index].reset_index(drop=True)\n",
    "    \n",
    "usr_comp_timestamp = usr_comp.merge(timestamp, how='left')\n",
    "usr_comp_timestamp = usr_comp_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_comp_timestamp.columns = ['User_ID', 'month', 'year', 'comp_timestamp']\n",
    "overall = overall.merge(usr_comp_timestamp, how='left')\n",
    "\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['comp_timestamp'] = overall.groupby('User_ID')['comp_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "overall['Months_Since_Last_Comp'] = overall['timestamp'] - overall['comp_timestamp']\n",
    "overall['Months_Since_Joining_Zindi'] = overall['comp_timestamp'] - overall['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitions = pd.read_csv(os.path.join(Config.DATA_DIR,\"Competitions.csv\"),\n",
    "                           index_col=None,\n",
    "                           skipinitialspace=True)\n",
    "competitions['CompEndTime Year'] = [int(val) if val!='not mapped' else 999 for val in competitions['CompEndTime Year']]\n",
    "competitions['FeatureC'] = competitions['FeatureC'].fillna(-1).astype(np.int8)\n",
    "competitions = competitions.merge(\n",
    "    timestamp,\n",
    "    left_on=['CompStartTime Year', 'CompStartTime Month'],\n",
    "    right_on=['year', 'month'],\n",
    "    how='left')\n",
    "competitions = competitions.rename(columns={\n",
    "    'timestamp': 'comp_start_timestamp',\n",
    "})\n",
    "competitions.drop(['year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "competitions = competitions.merge(\n",
    "    timestamp,\n",
    "    left_on=['CompEndTime Year', 'CompEndTime Month'],\n",
    "    right_on=['year', 'month'],\n",
    "    how='left')\n",
    "competitions = competitions.rename(columns={\n",
    "    'timestamp': 'comp_end_timestamp',\n",
    "})\n",
    "competitions['comp_end_timestamp'] = competitions['comp_end_timestamp'].fillna(99)\n",
    "competitions.drop(['year', 'month', 'year_month'], axis=1, inplace=True)\n",
    "competitions['comp_duration'] = competitions['comp_end_timestamp'] - competitions['comp_start_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "for col in ['FeatureA', 'FeatureB', 'FeatureE']:\n",
    "    competitions[col] = competitions[col].map(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_features = competitions[['CompID']].copy()\n",
    "for col in ['FeatureA', 'FeatureB', 'FeatureC', 'FeatureD', 'FeatureE']:\n",
    "    tmp = competitions[['CompID', col]].explode(column=[col])\n",
    "    tmp[col] = tmp[col].fillna('empty')\n",
    "    tmp['count'] = 1\n",
    "\n",
    "    tmp = tmp.pivot_table(index='CompID', \n",
    "                    columns=col,\n",
    "                    values='count',\n",
    "                    aggfunc='count')\n",
    "    tmp.columns = [tmp.columns.name + \"_\" + str(col) for col in tmp.columns]\n",
    "    tmp = tmp.reset_index()\n",
    "    comp_features = comp_features.merge(tmp, how='left')\n",
    "comp_features = comp_features.fillna(0)\n",
    "comp_features = comp_features.merge(competitions[['CompID', 'comp_start_timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time based competitions features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_ids = []\n",
    "competitions_ids = []\n",
    "for t in tqdm(timestamp.timestamp):\n",
    "    selected_comp = competitions.CompID[(t>=competitions.comp_start_timestamp)&(t<=competitions.comp_end_timestamp)]\n",
    "    timestamp_ids.extend([t]*len(selected_comp))\n",
    "    competitions_ids.extend(selected_comp)\n",
    "\n",
    "timestamp_comp = pd.DataFrame({\n",
    "    \"timestamp\": timestamp_ids,\n",
    "    \"CompID\": competitions_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current active competitions feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_comp = usr_comp.merge(timestamp, how='left')\n",
    "active_comp = active_comp.rename(columns={\"timestamp\": \"comp_timestamp\"})\n",
    "active_comp = active_comp[['User_ID', 'CompID', 'comp_timestamp']].merge(competitions[['CompID', 'comp_start_timestamp', 'comp_end_timestamp']], how='left')\n",
    "active_comp = active_comp[active_comp['comp_end_timestamp']!=99].reset_index(drop=True)\n",
    "active_comp = all_timestamps.merge(active_comp, how='left')\n",
    "\n",
    "active_comp['Current_Active_Competitions'] = (\n",
    "    (active_comp['timestamp'] > active_comp['comp_timestamp']) &\n",
    "    (active_comp['timestamp'] <= active_comp['comp_end_timestamp'])\n",
    ").astype(np.int8)\n",
    "\n",
    "active_comp = active_comp.groupby(['User_ID', 'timestamp'])['Current_Active_Competitions'].sum()\n",
    "active_comp = active_comp.reset_index()\n",
    "\n",
    "overall = overall.merge(active_comp, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interests Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_comp = timestamp_comp.merge(comp_features, how='left')\n",
    "timestamp_comp.drop(['comp_start_timestamp', 'CompID'], axis=1, inplace=True)\n",
    "timestamp_comp = timestamp_comp.groupby('timestamp').agg(np.sum).reset_index()\n",
    "timestamp_comp = timestamp_comp.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "usr_comp_timestamp = usr_comp.merge(timestamp, how='left')\n",
    "usr_comp_features = usr_comp_timestamp[['User_ID', 'CompID', 'timestamp']].merge(comp_features, how='left')\n",
    "usr_comp_features = usr_comp_features.drop(['CompID', 'comp_start_timestamp'], axis=1)\n",
    "usr_comp_features = usr_comp_features.groupby(['User_ID', 'timestamp']).agg(np.sum)#.groupby(level=0).cumsum()\n",
    "usr_comp_features = usr_comp_features.reset_index()\n",
    "usr_comp_features = all_timestamps.merge(usr_comp_features, how='left')\n",
    "sel_cols = usr_comp_features.columns[4:]\n",
    "print(sel_cols)\n",
    "\n",
    "usr_comp_features = usr_comp_features.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in tqdm(sel_cols):\n",
    "    usr_comp_features[col] = usr_comp_features.groupby('User_ID')[col].apply(lambda x: x.ffill())\n",
    "\n",
    "usr_comp_features = usr_comp_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "timestamp_ids = []\n",
    "usr_ids = []\n",
    "user_interests = []\n",
    "\n",
    "for t in timestamp_comp['timestamp']:\n",
    "    if t == 1:\n",
    "        continue\n",
    "    usr_f = usr_comp_features[usr_comp_features['timestamp']==t-1]\n",
    "    timestamp_ids.extend([t]*len(usr_f))\n",
    "    usr_ids.extend(usr_f.pop('User_ID'))\n",
    "    \n",
    "    comp_f = timestamp_comp[timestamp_comp['timestamp']==t]\n",
    "    usr_f.drop(['timestamp', 'year', 'month'], axis=1, inplace=True)\n",
    "    comp_f.drop('timestamp', axis=1, inplace=True)\n",
    "    \n",
    "    interests = np.matmul(usr_f.values, comp_f.values.T).flatten()\n",
    "#     interests = cosine_similarity(usr_f.values, comp_f.values).flatten()\n",
    "    user_interests.extend(interests)\n",
    "\n",
    "usr_interest_f = pd.DataFrame({\n",
    "    \"timestamp\": timestamp_ids,\n",
    "    \"User_ID\": usr_ids,\n",
    "    \"user_interests\": user_interests\n",
    "})\n",
    "\n",
    "overall = overall.merge(usr_interest_f, how='left')\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['user_interests'] = overall.groupby('User_ID')['user_interests'].apply(lambda x: x.ffill())\n",
    "overall['user_interests'] = overall['user_interests'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_hist = usr_comp.groupby(['User_ID', 'year', 'month'])['CompID'].nunique()\n",
    "comp_hist = comp_hist.reset_index()\n",
    "comp_hist.columns = [*comp_hist.columns[:-1]] + ['Num_Comp_Prev_Month']\n",
    "\n",
    "overall = overall.merge(comp_hist, how='left')\n",
    "overall = overall.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "overall['Num_Comp_Prev_Month'] = overall['Num_Comp_Prev_Month'].fillna(0)\n",
    "overall['Num_Comp_Per_Month'] = overall.groupby('User_ID')['Num_Comp_Prev_Month'].cumsum()\n",
    "overall['Num_Comp_Per_Month_trend'] = overall['Num_Comp_Per_Month']/overall['Total_Num_User_Months']\n",
    "overall['Num_Comp_Per_Month_trend'] = overall.groupby('User_ID')['Num_Comp_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "overall['Num_Comp_Per_Month'] = overall['Num_Comp_Per_Month']/(overall['timestamp'].max() - overall['Zindi_Joining_Timestamp'])\n",
    "overall['Num_Comp_Per_Month'] = overall.groupby('User_ID')['Num_Comp_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "overall['Num_Comp_Prev_Month'] = overall.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift())\n",
    "overall['Num_Comp_Prev_Month_momentum'] = overall['Num_Comp_Prev_Month'] - overall.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "overall['Num_Comp_Prev_Month_momentum2'] = overall['Num_Comp_Prev_Month'] - overall.groupby('User_ID')['Num_Comp_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = usr_comp.groupby(['User_ID', 'year', 'month', 'PublicRank'])['CompID'].nunique().unstack('PublicRank').apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [tmp.columns.name + \"_\" + str(col) for col in tmp.columns]\n",
    "tmp.columns = col_names\n",
    "tmp = tmp.fillna(0)\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "tmp = all_timestamps.merge(tmp, how='left')\n",
    "tmp = tmp.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    tmp[col] = tmp.groupby('User_ID')[col].apply(lambda x: x.cumsum().ffill().shift())\n",
    "    \n",
    "overall = overall.merge(tmp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = usr_comp.groupby(['User_ID', 'year', 'month', 'Successful_Sub_Count'])['CompID'].nunique().unstack('Successful_Sub_Count').apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [tmp.columns.name + \"_\" + str(col) for col in tmp.columns]\n",
    "tmp.columns = col_names\n",
    "tmp = tmp.fillna(0)\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "tmp = all_timestamps.merge(tmp, how='left')\n",
    "tmp = tmp.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    tmp[col] = tmp.groupby('User_ID')[col].apply(lambda x: x.cumsum().ffill().shift())\n",
    "    \n",
    "overall = overall.merge(tmp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_comp, usr_comp_timestamp, comp_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_sub = pd.read_csv(os.path.join(Config.DATA_DIR,\"Submissions.csv\"), index_col=None)\n",
    "usr_sub.columns = ['User_ID', 'FeatureG', 'CompID', 'year', 'month', 'dayofweek']\n",
    "\n",
    "if LOCAL_TEST_RUN:\n",
    "    test_index = (usr_sub['year']==3)&(usr_sub['month'].isin([10,11,12]))\n",
    "    usr_sub = usr_sub[~test_index].reset_index(drop=True)\n",
    "    \n",
    "usr_sub_timestamp = usr_sub.merge(timestamp, how='left')\n",
    "usr_sub_timestamp = usr_sub_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_sub_timestamp.columns = ['User_ID', 'month', 'year', 'sub_timestamp']\n",
    "overall = overall.merge(usr_sub_timestamp, how='left')\n",
    "\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['sub_timestamp'] = overall.groupby('User_ID')['sub_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "overall['Months_Since_Last_Sub'] = overall['timestamp'] - overall['sub_timestamp']\n",
    "overall['Months_Since_Sub_Joining_Zindi'] = overall['sub_timestamp'] - overall['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_hist = usr_sub.groupby(['User_ID', 'year', 'month']).agg({'CompID': ['nunique', 'count']})\n",
    "sub_hist.columns = [\"_\".join(col) for col in sub_hist.columns]\n",
    "sub_hist['Sub_Per_Comp'] = sub_hist['CompID_nunique']/sub_hist['CompID_count']\n",
    "sub_hist.drop(['CompID_nunique', 'CompID_count'], axis=1, inplace=True)\n",
    "sub_hist = sub_hist.reset_index()\n",
    "sub_hist.columns = [*sub_hist.columns[:-1]] + ['Num_Sub_Prev_Month']\n",
    "\n",
    "overall = overall.merge(sub_hist, how='left')\n",
    "overall = overall.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "overall['Num_Sub_Prev_Month'] = overall['Num_Sub_Prev_Month'].fillna(0)\n",
    "overall['Num_Sub_Per_Month'] = overall.groupby('User_ID')['Num_Sub_Prev_Month'].cumsum()\n",
    "overall['Num_Sub_Per_Month_trend'] = overall['Num_Sub_Per_Month']/overall['Total_Num_User_Months']\n",
    "overall['Num_Sub_Per_Month_trend'] = overall.groupby('User_ID')['Num_Sub_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "overall['Num_Sub_Per_Month'] = overall['Num_Sub_Per_Month']/(overall['timestamp'].max() - overall['Zindi_Joining_Timestamp'])\n",
    "overall['Num_Sub_Per_Month'] = overall.groupby('User_ID')['Num_Sub_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "overall['Num_Sub_Prev_Month'] = overall.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift())\n",
    "overall['Num_Sub_Prev_Month_momentum'] = overall['Num_Sub_Prev_Month'] - overall.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "overall['Num_Sub_Prev_Month_momentum2'] = overall['Num_Sub_Prev_Month'] - overall.groupby('User_ID')['Num_Sub_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = usr_sub.groupby(['User_ID', 'year', 'month', 'FeatureG'])['CompID'].nunique().unstack('FeatureG')#.apply(lambda x: x/x.sum(), axis=1)\n",
    "col_names = [tmp.columns.name + \"_\" + str(col) for col in tmp.columns]\n",
    "tmp.columns = col_names\n",
    "tmp = tmp.fillna(0)\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "all_timestamps = overall[['User_ID', 'timestamp', 'year', 'month']].drop_duplicates().reset_index(drop=True)\n",
    "tmp = all_timestamps.merge(tmp, how='left')\n",
    "tmp = tmp.sort_values(by='timestamp').reset_index(drop=True)\n",
    "for col in col_names:\n",
    "    tmp[col] = tmp.groupby('User_ID')[col].apply(lambda x: x.ffill().shift())\n",
    "    \n",
    "overall = overall.merge(tmp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_sub, usr_sub_timestamp, sub_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dis = pd.read_csv(os.path.join(Config.DATA_DIR,\"Discussions.csv\"), index_col=None)\n",
    "usr_dis.columns = ['FeatureF', 'year', 'month', 'dayofweek', 'DiscID', 'User_ID']\n",
    "\n",
    "if LOCAL_TEST_RUN:\n",
    "    test_index = (usr_dis['year']==3)&(usr_dis['month'].isin([10,11,12]))\n",
    "    usr_dis = usr_dis[~test_index].reset_index(drop=True)\n",
    "    \n",
    "usr_dis_timestamp = usr_dis.merge(timestamp, how='left')\n",
    "usr_dis_timestamp = usr_dis_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_dis_timestamp.columns = ['User_ID', 'month', 'year', 'discussion_timestamp']\n",
    "overall = overall.merge(usr_dis_timestamp, how='left')\n",
    "\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['discussion_timestamp'] = overall.groupby('User_ID')['discussion_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "overall['Months_Since_Last_Dis'] = overall['timestamp'] - overall['discussion_timestamp']\n",
    "overall['Months_Since_Dis_Joining_Zindi'] = overall['discussion_timestamp'] - overall['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_hist = usr_dis.groupby(['User_ID', 'year', 'month'])['DiscID'].nunique()\n",
    "dis_hist = dis_hist.reset_index()\n",
    "dis_hist.columns = [*dis_hist.columns[:-1]] + ['Num_Dis_Prev_Month']\n",
    "\n",
    "overall = overall.merge(dis_hist, how='left')\n",
    "overall = overall.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "overall['Num_Dis_Prev_Month'] = overall['Num_Dis_Prev_Month'].fillna(0)\n",
    "overall['Num_Dis_Per_Month'] = overall.groupby('User_ID')['Num_Dis_Prev_Month'].cumsum()\n",
    "overall['Num_Dis_Per_Month_trend'] = overall['Num_Dis_Per_Month']/overall['Total_Num_User_Months']\n",
    "overall['Num_Dis_Per_Month_trend'] = overall.groupby('User_ID')['Num_Dis_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "overall['Num_Dis_Per_Month'] = overall['Num_Dis_Per_Month']/(overall['timestamp'].max() - overall['Zindi_Joining_Timestamp'])\n",
    "overall['Num_Dis_Per_Month'] = overall.groupby('User_ID')['Num_Dis_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "overall['Num_Dis_Prev_Month'] = overall.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift())\n",
    "overall['Num_Dis_Prev_Month_momentum'] = overall['Num_Dis_Prev_Month'] - overall.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "overall['Num_Dis_Prev_Month_momentum2'] = overall['Num_Dis_Prev_Month'] - overall.groupby('User_ID')['Num_Dis_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_dis, usr_dis_timestamp, dis_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_comments = pd.read_csv(os.path.join(Config.DATA_DIR,\"Comments.csv\"), index_col=None)\n",
    "usr_comments.columns = ['User_ID', 'year', 'month', 'dayofweek']\n",
    "usr_comments['CommID'] = np.arange(len(usr_comments))\n",
    "\n",
    "if LOCAL_TEST_RUN:\n",
    "    test_index = (usr_comments['year']==3)&(usr_comments['month'].isin([10,11,12]))\n",
    "    usr_comments = usr_comments[~test_index].reset_index(drop=True)\n",
    "    \n",
    "usr_comm_timestamp = usr_comments.merge(timestamp, how='left')\n",
    "usr_comm_timestamp = usr_comm_timestamp[['User_ID', 'month', 'year', 'timestamp']].drop_duplicates()\n",
    "usr_comm_timestamp.columns = ['User_ID', 'month', 'year', 'comment_timestamp']\n",
    "overall = overall.merge(usr_comm_timestamp, how='left')\n",
    "\n",
    "overall = overall.sort_values(by='timestamp').reset_index(drop=True)\n",
    "overall['comment_timestamp'] = overall.groupby('User_ID')['comment_timestamp'].apply(lambda x: x.ffill().shift())\n",
    "overall['Months_Since_Last_Comment'] = overall['timestamp'] - overall['comment_timestamp']\n",
    "overall['Months_Since_Comment_Joining_Zindi'] = overall['comment_timestamp'] - overall['Zindi_Joining_Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_hist = usr_comments.groupby(['User_ID', 'year', 'month'])['CommID'].nunique()\n",
    "comm_hist = comm_hist.reset_index()\n",
    "comm_hist.columns = [*comm_hist.columns[:-1]] + ['Num_Comm_Prev_Month']\n",
    "\n",
    "overall = overall.merge(comm_hist, how='left')\n",
    "overall = overall.sort_values(by=['User_ID', 'timestamp']).reset_index(drop=True)\n",
    "overall['Num_Comm_Prev_Month'] = overall['Num_Comm_Prev_Month'].fillna(0)\n",
    "overall['Num_Comm_Per_Month'] = overall.groupby('User_ID')['Num_Comm_Prev_Month'].cumsum()\n",
    "overall['Num_Comm_Per_Month_trend'] = overall['Num_Comm_Per_Month']/overall['Total_Num_User_Months']\n",
    "overall['Num_Comm_Per_Month_trend'] = overall.groupby('User_ID')['Num_Comm_Per_Month_trend'].apply(lambda x: x.shift())\n",
    "overall['Num_Comm_Per_Month'] = overall['Num_Comm_Per_Month']/(overall['timestamp'].max() - overall['Zindi_Joining_Timestamp'])\n",
    "overall['Num_Comm_Per_Month'] = overall.groupby('User_ID')['Num_Comm_Per_Month'].apply(lambda x: x.shift())\n",
    "\n",
    "overall['Num_Comm_Prev_Month'] = overall.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift())\n",
    "overall['Num_Comm_Prev_Month_momentum'] = overall['Num_Comm_Prev_Month'] - overall.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift(1))\n",
    "overall['Num_Comm_Prev_Month_momentum2'] = overall['Num_Comm_Prev_Month'] - overall.groupby('User_ID')['Num_Comm_Prev_Month'].apply(lambda x: x.shift(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del usr_comments, usr_comm_timestamp, comm_hist\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = overall.groupby('timestamp').agg({\n",
    "    \"User_ID\": [\"nunique\"],\n",
    "    \"Total_Num_User_Months\": [\"mean\", \"max\", \"std\"],\n",
    "})\n",
    "tmp.columns = [\"_\".join(col) for col in tmp.columns]\n",
    "tmp = tmp.reset_index()\n",
    "\n",
    "overall = overall.merge(tmp, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cols = ['Months_Since_Last_Comp', 'Months_Since_Last_Dis', 'Months_Since_Last_Sub', 'Months_Since_Last_Comment']\n",
    "overall['Months_Since_Last_Activity_Mean'] = overall[sel_cols].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = [\n",
    "    'Zindi_Joining_Timestamp',\n",
    "    'comment_timestamp',\n",
    "    'comp_timestamp',\n",
    "    'discussion_timestamp',\n",
    "    'sub_timestamp',\n",
    "    'Months_Since_Last_Comp',\n",
    "    'Months_Since_Last_Sub',\n",
    "    'Months_Since_Last_Dis',\n",
    "    'Months_Since_Last_Comment',\n",
    "]\n",
    "\n",
    "for col in time_cols:\n",
    "    overall[col] = overall[col]/overall['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_time = overall[overall['Zindi_Joining_Timestamp']==1]\n",
    "tmp_time = tmp_time.groupby('timestamp')['User_ID'].nunique().to_frame(\"unique_user_count\")\n",
    "tmp_time = tmp_time.reset_index()\n",
    "\n",
    "overall = overall.merge(tmp_time, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall['user_interests_rank'] = overall.groupby('timestamp')['user_interests'].apply(lambda x: \n",
    "                                                                                      x.rank(method='dense', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall.loc[overall['user_interests']==0, 'user_interests'] = np.NaN\n",
    "print(train.shape, test.shape)\n",
    "train, test = overall[overall['is_train']==1], overall[overall['is_train']==0]\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df_trainX, df_trainY, df_evalX, df_evalY, cat_cols, model_name='CAT', params=None):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    if model_name == 'CAT':\n",
    "        if params is None:\n",
    "            params={'n_estimators':10000,'random_state':123,'cat_features':cat_cols}\n",
    "        clf=CatBoostClassifier(**params,early_stopping_rounds=50,eval_metric='AUC')\n",
    "        clf.fit(df_trainX,df_trainY,eval_set=(df_evalX,df_evalY),plot=False, verbose=50)\n",
    "        valid_score = clf.get_best_score().get('validation').get('AUC')\n",
    "        best_iteration = clf.get_best_iteration()\n",
    "        feature_score = clf.get_feature_importance()\n",
    "    elif model_name == 'LGB':\n",
    "        if params is None:\n",
    "            params={'verbose':0,'n_estimators':10000,'random_state':123,'learning_rate':0.01,'force_row_wise':True,'colsample_bytree':0.3}\n",
    "        clf = lgb.LGBMClassifier(**params, importance_type='gain', metric='auc_mu', num_leaves=127, min_child_samples=5)\n",
    "        callbacks = [lgb.early_stopping(500, verbose=0)]\n",
    "        clf.fit(df_trainX,\n",
    "                df_trainY,#)\n",
    "                eval_set=[(df_evalX, df_evalY)],\n",
    "                callbacks=callbacks,\n",
    "                # verbose=0\n",
    "               )\n",
    "\n",
    "        valid_score = roc_auc_score(df_evalY!='NoActivity', 1-clf.predict_proba(df_evalX)[:,1])\n",
    "        best_iteration = clf.booster_.best_iteration\n",
    "        feature_score = clf.feature_importances_\n",
    "    return clf, valid_score, best_iteration, feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(Config.DATA_DIR,\"Train_fe.csv.gz\"), compression='gzip')\n",
    "test.to_csv(os.path.join(Config.DATA_DIR,\"Test_fe.csv.gz\"), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drop_cols = [\n",
    "    'year', 'month', 'Target', 'Sub', 'CompPart', 'Comment', 'Disc',\n",
    "    'is_train', 'timestamp', 'Record', 'Active_Month', 'Total_Num_User_Months',\n",
    "    'user_interests'\n",
    "]\n",
    "cat_cols = list(\n",
    "    set(train.columns[train.dtypes == 'object']) - set(drop_cols) - set(['User_ID'])\n",
    ")\n",
    "num_cols = list(set(train.columns) - set(cat_cols + drop_cols))\n",
    "\n",
    "train_X = train[cat_cols + num_cols]\n",
    "train_X[cat_cols] = train_X[cat_cols].astype('category')\n",
    "train_Y = train['Target']\n",
    "\n",
    "test_X = test[cat_cols + num_cols]\n",
    "test_X[cat_cols] = test_X[cat_cols].astype('category')\n",
    "\n",
    "fold = GroupKFold(n_splits=5)\n",
    "cb_scores, pred_cb, feat_scores = [], [], []\n",
    "for it, (idxT, idxV) in enumerate(\n",
    "        fold.split(train_X, train_Y, groups=train['timestamp'])):\n",
    "    df_trainX, df_trainY = train_X.iloc[idxT], train_Y.iloc[idxT]\n",
    "    df_evalX, df_evalY = train_X.iloc[idxV], train_Y.iloc[idxV]\n",
    "    df_testX = test_X.copy()\n",
    "\n",
    "    selected_cat_cols = ['Country']\n",
    "    cat_cols_count = [f'{col}_count' for col in selected_cat_cols]\n",
    "    df_trainX[cat_cols_count] = df_trainX[selected_cat_cols].copy()\n",
    "    df_evalX[cat_cols_count] = df_evalX[selected_cat_cols].copy()\n",
    "    df_testX[cat_cols_count] = df_testX[selected_cat_cols].copy()\n",
    "\n",
    "    encoder = CountEncoder(cols=cat_cols_count + ['User_ID'])\n",
    "    df_trainX = encoder.fit_transform(df_trainX, df_trainY)\n",
    "    df_evalX = encoder.transform(df_evalX)\n",
    "    df_testX = encoder.transform(df_testX)\n",
    "\n",
    "    clf, valid_score, best_iteration, feature_score = train_model(\n",
    "        df_trainX, df_trainY, df_evalX, df_evalY, cat_cols, model_name='LGB')\n",
    "    cb_scores.append(valid_score)\n",
    "    pred_cb.append(clf.predict_proba(df_testX)[:, 1])\n",
    "    feat_scores.append(feature_score)\n",
    "    print('Fold {} {} at {}'.format(it + 1, valid_score, best_iteration))\n",
    "\n",
    "weights = cb_scores / sum(np.array(cb_scores))\n",
    "print('The local CV is {}'.format(np.sum(weights * cb_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_TEST_RUN:\n",
    "    weights=cb_scores/sum(np.array(cb_scores))\n",
    "    print ('The local CV is {}'.format(np.sum(weights*cb_scores)))\n",
    "\n",
    "    prediction = np.sum(weights*np.transpose(pred_cb),1)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    print(\"Test score is {}\".format(roc_auc_score(test['Target']!='NoActivity', 1-prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "featureImp=pd.DataFrame({'feature':df_trainX.columns,'importance':np.mean(np.array(feat_scores),0)})\n",
    "featureImp=featureImp.sort_values('importance',ascending=False)\n",
    "featureImp['importance']=featureImp['importance']*100/featureImp['importance'].sum()\n",
    "featureImp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Target'] = np.sum(weights * np.transpose(pred_cb), 1)\n",
    "test['Target'] = 1 - test['Target']\n",
    "test['UserMonthYear'] = test['User_ID'] + \"_\" + test['month'].astype(str) + \"_\" + test['year'].astype(str)\n",
    "test[['UserMonthYear', 'Target']].to_csv(os.path.join(Config.OUTPUT_DIR, f'{Config.VER}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['UserMonthYear', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
